root: INFO: Namespace(arch='alexnet', batch_size=64, bits='8,16,24,32', epochs=3, gamma=200, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 8
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 27.6223]
root: INFO: [Iteration:   1/ 50][Train Loss: 14.4680]
root: INFO: [Iteration:   2/ 50][Train Loss: 13.4814]
root: INFO: [Iteration:   3/ 50][Train Loss: 13.0978]
root: INFO: [Iteration:   4/ 50][Train Loss: 12.9502]
root: INFO: [Iteration:   5/ 50][Train Loss: 12.4163]
root: INFO: [Iteration:   6/ 50][Train Loss: 12.6552]
root: INFO: [Iteration:   7/ 50][Train Loss: 12.8331]
root: INFO: [Iteration:   8/ 50][Train Loss: 12.6951]
root: INFO: [Iteration:   9/ 50][Train Loss: 12.6059]
root: INFO: [Iteration:  10/ 50][Train Loss: 12.3803]
root: INFO: [Iteration:  11/ 50][Train Loss: 13.5039]
root: INFO: [Iteration:  12/ 50][Train Loss: 13.4717]
root: INFO: [Iteration:  13/ 50][Train Loss: 13.2118]
root: INFO: [Iteration:  14/ 50][Train Loss: 13.4692]
root: INFO: [Iteration:  15/ 50][Train Loss: 13.1887]
root: INFO: [Iteration:  16/ 50][Train Loss: 13.5467]
root: INFO: [Iteration:  17/ 50][Train Loss: 12.9608]
root: INFO: [Iteration:  18/ 50][Train Loss: 13.1288]
root: INFO: [Iteration:  19/ 50][Train Loss: 12.9052]
root: INFO: [Iteration:  20/ 50][Train Loss: 12.6753]
root: INFO: [Iteration:  21/ 50][Train Loss: 13.8153]
root: INFO: [Iteration:  22/ 50][Train Loss: 13.4535]
root: INFO: [Iteration:  23/ 50][Train Loss: 13.8476]
root: INFO: [Iteration:  24/ 50][Train Loss: 13.3240]
root: INFO: [Iteration:  25/ 50][Train Loss: 13.1837]
root: INFO: [Iteration:  26/ 50][Train Loss: 13.5787]
root: INFO: [Iteration:  27/ 50][Train Loss: 13.1252]
root: INFO: [Iteration:  28/ 50][Train Loss: 13.0265]
root: INFO: [Iteration:  29/ 50][Train Loss: 13.3281]
root: INFO: [Iteration:  30/ 50][Train Loss: 13.6859]
root: INFO: [Iteration:  31/ 50][Train Loss: 13.5559]
root: INFO: [Iteration:  32/ 50][Train Loss: 13.3953]
root: INFO: [Iteration:  33/ 50][Train Loss: 13.1876]
root: INFO: [Iteration:  34/ 50][Train Loss: 12.9435]
root: INFO: [Iteration:  35/ 50][Train Loss: 13.1990]
root: INFO: [Iteration:  36/ 50][Train Loss: 12.9270]
root: INFO: [Iteration:  37/ 50][Train Loss: 13.5370]
root: INFO: [Iteration:  38/ 50][Train Loss: 12.8369]
root: INFO: [Iteration:  39/ 50][Train Loss: 13.0712]
root: INFO: [Iteration:  40/ 50][Train Loss: 12.9211]
root: INFO: [Iteration:  41/ 50][Train Loss: 13.0622]
root: INFO: [Iteration:  42/ 50][Train Loss: 13.4984]
root: INFO: [Iteration:  43/ 50][Train Loss: 13.6655]
root: INFO: [Iteration:  44/ 50][Train Loss: 13.3512]
root: INFO: [Iteration:  45/ 50][Train Loss: 13.1934]
root: INFO: [Iteration:  46/ 50][Train Loss: 13.2684]
root: INFO: [Iteration:  47/ 50][Train Loss: 13.5167]
root: INFO: [Iteration:  48/ 50][Train Loss: 12.8199]
root: INFO: [Iteration:  49/ 50][Train Loss: 13.4957]
root: INFO: [Evaluation: old top-50 mAP: 0.8811, top-50 mAP: 0.8338, top-100 NDCG: 0.3391, top-100 ACG: 1.6536]
root: INFO: Namespace(arch='alexnet', batch_size=64, bits='8,16,24,32', epochs=3, gamma=200, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 16
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 102.2126]
root: INFO: [Iteration:   1/ 50][Train Loss: 53.0008]
root: INFO: [Iteration:   2/ 50][Train Loss: 51.8548]
root: INFO: [Iteration:   3/ 50][Train Loss: 48.1752]
root: INFO: [Iteration:   4/ 50][Train Loss: 49.4340]
root: INFO: [Iteration:   5/ 50][Train Loss: 46.8881]
root: INFO: [Iteration:   6/ 50][Train Loss: 49.1844]
root: INFO: [Iteration:   7/ 50][Train Loss: 46.9048]
root: INFO: [Iteration:   8/ 50][Train Loss: 45.7450]
root: INFO: [Iteration:   9/ 50][Train Loss: 47.0500]
root: INFO: [Iteration:  10/ 50][Train Loss: 48.3304]
root: INFO: [Iteration:  11/ 50][Train Loss: 50.4167]
root: INFO: [Iteration:  12/ 50][Train Loss: 49.2956]
root: INFO: [Iteration:  13/ 50][Train Loss: 51.7398]
root: INFO: [Iteration:  14/ 50][Train Loss: 50.7191]
root: INFO: [Iteration:  15/ 50][Train Loss: 50.1616]
root: INFO: [Iteration:  16/ 50][Train Loss: 49.0740]
root: INFO: [Iteration:  17/ 50][Train Loss: 48.7941]
root: INFO: [Iteration:  18/ 50][Train Loss: 50.5548]
root: INFO: [Iteration:  19/ 50][Train Loss: 50.6020]
root: INFO: [Iteration:  20/ 50][Train Loss: 48.0267]
root: INFO: [Iteration:  21/ 50][Train Loss: 50.3446]
root: INFO: [Iteration:  22/ 50][Train Loss: 50.5715]
root: INFO: [Iteration:  23/ 50][Train Loss: 50.8659]
root: INFO: [Iteration:  24/ 50][Train Loss: 53.5148]
root: INFO: [Iteration:  25/ 50][Train Loss: 52.2301]
root: INFO: [Iteration:  26/ 50][Train Loss: 50.8263]
root: INFO: [Iteration:  27/ 50][Train Loss: 50.4266]
root: INFO: [Iteration:  28/ 50][Train Loss: 50.1604]
root: INFO: [Iteration:  29/ 50][Train Loss: 50.1650]
root: INFO: [Iteration:  30/ 50][Train Loss: 51.5584]
root: INFO: [Iteration:  31/ 50][Train Loss: 51.3290]
root: INFO: [Iteration:  32/ 50][Train Loss: 51.0673]
root: INFO: [Iteration:  33/ 50][Train Loss: 51.9669]
root: INFO: [Iteration:  34/ 50][Train Loss: 50.7943]
root: INFO: [Iteration:  35/ 50][Train Loss: 51.7516]
root: INFO: [Iteration:  36/ 50][Train Loss: 50.0510]
root: INFO: [Iteration:  37/ 50][Train Loss: 49.1789]
root: INFO: [Iteration:  38/ 50][Train Loss: 50.9117]
root: INFO: [Iteration:  39/ 50][Train Loss: 50.2566]
root: INFO: [Iteration:  40/ 50][Train Loss: 50.0028]
root: INFO: [Iteration:  41/ 50][Train Loss: 50.7891]
root: INFO: [Iteration:  42/ 50][Train Loss: 51.2313]
root: INFO: [Iteration:  43/ 50][Train Loss: 51.6473]
root: INFO: [Iteration:  44/ 50][Train Loss: 49.4154]
root: INFO: [Iteration:  45/ 50][Train Loss: 51.1839]
root: INFO: [Iteration:  46/ 50][Train Loss: 50.4342]
root: INFO: [Iteration:  47/ 50][Train Loss: 51.9846]
root: INFO: [Iteration:  48/ 50][Train Loss: 51.3844]
root: INFO: [Iteration:  49/ 50][Train Loss: 50.4774]
root: INFO: [Evaluation: old top-50 mAP: 0.9074, top-50 mAP: 0.8783, top-100 NDCG: 0.3916, top-100 ACG: 1.8093]
root: INFO: Namespace(arch='alexnet', batch_size=64, bits='8,16,24,32', epochs=3, gamma=200, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 24
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 227.5104]
root: INFO: [Iteration:   1/ 50][Train Loss: 162.0829]
root: INFO: [Iteration:   2/ 50][Train Loss: 121.1311]
root: INFO: [Iteration:   3/ 50][Train Loss: 127.8928]
root: INFO: [Iteration:   4/ 50][Train Loss: 113.8041]
root: INFO: [Iteration:   5/ 50][Train Loss: 110.4582]
root: INFO: [Iteration:   6/ 50][Train Loss: 114.2464]
root: INFO: [Iteration:   7/ 50][Train Loss: 110.2739]
root: INFO: [Iteration:   8/ 50][Train Loss: 109.8476]
root: INFO: [Iteration:   9/ 50][Train Loss: 109.9809]
root: INFO: [Iteration:  10/ 50][Train Loss: 112.1205]
root: INFO: [Iteration:  11/ 50][Train Loss: 113.3885]
root: INFO: [Iteration:  12/ 50][Train Loss: 110.3978]
root: INFO: [Iteration:  13/ 50][Train Loss: 114.4220]
root: INFO: [Iteration:  14/ 50][Train Loss: 111.7625]
root: INFO: [Iteration:  15/ 50][Train Loss: 112.1977]
root: INFO: [Iteration:  16/ 50][Train Loss: 114.0954]
root: INFO: [Iteration:  17/ 50][Train Loss: 108.1343]
root: INFO: [Iteration:  18/ 50][Train Loss: 109.7049]
root: INFO: [Iteration:  19/ 50][Train Loss: 108.8587]
root: INFO: [Iteration:  20/ 50][Train Loss: 111.5297]
root: INFO: [Iteration:  21/ 50][Train Loss: 116.7008]
root: INFO: [Iteration:  22/ 50][Train Loss: 113.3740]
root: INFO: [Iteration:  23/ 50][Train Loss: 114.8091]
root: INFO: [Iteration:  24/ 50][Train Loss: 116.6246]
root: INFO: [Iteration:  25/ 50][Train Loss: 116.9407]
root: INFO: [Iteration:  26/ 50][Train Loss: 115.8777]
root: INFO: [Iteration:  27/ 50][Train Loss: 111.1796]
root: INFO: [Iteration:  28/ 50][Train Loss: 118.0049]
root: INFO: [Iteration:  29/ 50][Train Loss: 115.5633]
root: INFO: [Iteration:  30/ 50][Train Loss: 116.1293]
root: INFO: [Iteration:  31/ 50][Train Loss: 115.6575]
root: INFO: [Iteration:  32/ 50][Train Loss: 114.1828]
root: INFO: [Iteration:  33/ 50][Train Loss: 114.8291]
root: INFO: [Iteration:  34/ 50][Train Loss: 114.7134]
root: INFO: [Iteration:  35/ 50][Train Loss: 111.8843]
root: INFO: [Iteration:  36/ 50][Train Loss: 118.5609]
root: INFO: [Iteration:  37/ 50][Train Loss: 113.8265]
root: INFO: [Iteration:  38/ 50][Train Loss: 120.1807]
root: INFO: [Iteration:  39/ 50][Train Loss: 112.5813]
root: INFO: [Iteration:  40/ 50][Train Loss: 112.7145]
root: INFO: [Iteration:  41/ 50][Train Loss: 112.9279]
root: INFO: [Iteration:  42/ 50][Train Loss: 115.6279]
root: INFO: [Iteration:  43/ 50][Train Loss: 116.2366]
root: INFO: [Iteration:  44/ 50][Train Loss: 115.7197]
root: INFO: [Iteration:  45/ 50][Train Loss: 112.7289]
root: INFO: [Iteration:  46/ 50][Train Loss: 120.8591]
root: INFO: [Iteration:  47/ 50][Train Loss: 112.1154]
root: INFO: [Iteration:  48/ 50][Train Loss: 112.2361]
root: INFO: [Iteration:  49/ 50][Train Loss: 116.9713]
root: INFO: [Evaluation: old top-50 mAP: 0.9167, top-50 mAP: 0.8913, top-100 NDCG: 0.4575, top-100 ACG: 2.0671]
root: INFO: Namespace(arch='alexnet', batch_size=64, bits='8,16,24,32', epochs=3, gamma=200, gpu='0', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 32
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 405.7968]
root: INFO: [Iteration:   1/ 50][Train Loss: 420.3903]
root: INFO: [Iteration:   2/ 50][Train Loss: 412.6022]
root: INFO: [Iteration:   3/ 50][Train Loss: 414.4013]
root: INFO: [Iteration:   4/ 50][Train Loss: 413.5594]
root: INFO: [Iteration:   5/ 50][Train Loss: 423.0142]
root: INFO: [Iteration:   6/ 50][Train Loss: 409.5837]
root: INFO: [Iteration:   7/ 50][Train Loss: 415.2159]
root: INFO: [Iteration:   8/ 50][Train Loss: 406.5311]
root: INFO: [Iteration:   9/ 50][Train Loss: 411.6615]
root: INFO: [Iteration:  10/ 50][Train Loss: 414.4998]
root: INFO: [Iteration:  11/ 50][Train Loss: 416.4744]
root: INFO: [Iteration:  12/ 50][Train Loss: 422.0002]
root: INFO: [Iteration:  13/ 50][Train Loss: 407.5517]
root: INFO: [Iteration:  14/ 50][Train Loss: 414.3960]
root: INFO: [Iteration:  15/ 50][Train Loss: 417.9278]
root: INFO: [Iteration:  16/ 50][Train Loss: 412.9899]
root: INFO: [Iteration:  17/ 50][Train Loss: 417.1116]
root: INFO: [Iteration:  18/ 50][Train Loss: 408.6089]
root: INFO: [Iteration:  19/ 50][Train Loss: 412.7511]
root: INFO: [Iteration:  20/ 50][Train Loss: 412.1013]
root: INFO: [Iteration:  21/ 50][Train Loss: 413.1898]
root: INFO: [Iteration:  22/ 50][Train Loss: 402.3445]
root: INFO: [Iteration:  23/ 50][Train Loss: 417.3620]
root: INFO: [Iteration:  24/ 50][Train Loss: 410.6483]
root: INFO: [Iteration:  25/ 50][Train Loss: 415.5690]
root: INFO: [Iteration:  26/ 50][Train Loss: 419.4884]
root: INFO: [Iteration:  27/ 50][Train Loss: 417.6095]
root: INFO: [Iteration:  28/ 50][Train Loss: 415.7618]
root: INFO: [Iteration:  29/ 50][Train Loss: 421.8657]
root: INFO: [Iteration:  30/ 50][Train Loss: 412.6546]
root: INFO: [Iteration:  31/ 50][Train Loss: 412.4761]
root: INFO: [Iteration:  32/ 50][Train Loss: 423.1820]
root: INFO: [Iteration:  33/ 50][Train Loss: 415.6185]
root: INFO: [Iteration:  34/ 50][Train Loss: 422.0114]
root: INFO: [Iteration:  35/ 50][Train Loss: 413.3758]
root: INFO: [Iteration:  36/ 50][Train Loss: 418.1896]
root: INFO: [Iteration:  37/ 50][Train Loss: 414.4207]
root: INFO: [Iteration:  38/ 50][Train Loss: 413.9083]
root: INFO: [Iteration:  39/ 50][Train Loss: 414.1047]
root: INFO: [Iteration:  40/ 50][Train Loss: 411.8465]
root: INFO: [Iteration:  41/ 50][Train Loss: 408.0413]
root: INFO: [Iteration:  42/ 50][Train Loss: 414.1835]
root: INFO: [Iteration:  43/ 50][Train Loss: 411.4880]
root: INFO: [Iteration:  44/ 50][Train Loss: 405.9937]
root: INFO: [Iteration:  45/ 50][Train Loss: 419.2835]
root: INFO: [Iteration:  46/ 50][Train Loss: 414.0030]
root: INFO: [Iteration:  47/ 50][Train Loss: 410.5070]
root: INFO: [Iteration:  48/ 50][Train Loss: 422.1471]
root: INFO: [Iteration:  49/ 50][Train Loss: 411.4371]
root: INFO: [Evaluation: old top-50 mAP: 0.8298, top-50 mAP: 0.7672, top-100 NDCG: 0.2479, top-100 ACG: 1.2358]
